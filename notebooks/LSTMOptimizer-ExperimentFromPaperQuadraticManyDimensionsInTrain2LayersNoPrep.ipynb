{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla M2090 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne as L\n",
    "\n",
    "from theano.printing import Print as TPP\n",
    "\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run stochastic_lstm_optimizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMOptimizer:\n",
    "    def __init__(self, input_var, func, func_params, loss_type='sum', **kwargs):\n",
    "        n_steps = T.iscalar()\n",
    "        \n",
    "        l_input = L.layers.InputLayer(shape=(None,), input_var=input_var)\n",
    "        self.l_optim = LSTMOptimizerLayer(l_input, n_steps=n_steps, function=func, **kwargs)\n",
    "        \n",
    "        self.params_init = L.layers.get_all_param_values(self.l_optim)\n",
    "\n",
    "        theta_history, loss_history = L.layers.get_output(self.l_optim)\n",
    "        if loss_type == 'sum':\n",
    "            loss = loss_history.sum()\n",
    "        else:\n",
    "            loss = np.log(loss_history).sum()\n",
    "        \n",
    "        self.lr = theano.shared(np.array(0.01, dtype=np.float32))\n",
    "\n",
    "        params = L.layers.get_all_params(self.l_optim)\n",
    "        updates = L.updates.adam(loss, params, learning_rate=self.lr)\n",
    "        \n",
    "        self.loss_fn = theano.function([input_var, n_steps] + func_params, [theta_history, loss_history], allow_input_downcast=True)\n",
    "        self.train_fn = theano.function([input_var, n_steps] + func_params, [theta_history, loss_history], updates=updates, allow_input_downcast=True)\n",
    "        \n",
    "    def reset_network(self):\n",
    "        L.layers.set_all_param_values(self.l_optim, self.params_init)\n",
    "        \n",
    "    def train(self, sample_function, n_iter=100, n_epochs=50, batch_size=100, decay_rate=0.96):\n",
    "        for i in range(n_epochs):\n",
    "            t = time.time()    \n",
    "\n",
    "            training_loss_history = []\n",
    "            for j in range(batch_size):\n",
    "                theta, params = sample_function()\n",
    " \n",
    "                theta_history, loss_history = self.train_fn(theta, n_iter, *params)\n",
    "                loss = sum(loss_history)\n",
    "                training_loss_history.append(loss_history)\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "            plt.semilogy(np.median(training_loss_history, axis=0))\n",
    "            plt.show()\n",
    "            print(\"Epoch number {}\".format(i))\n",
    "            t = time.time() - t\n",
    "\n",
    "            print(\"Time on epoch {}: {}\".format(i, t))\n",
    "            print(\"Loss on {}: {}; {}\".format(i, loss, np.median(training_loss_history, axis=0)[-1]))\n",
    "\n",
    "            self.lr.set_value((self.lr.get_value() * decay_rate).astype(np.float32))\n",
    "            \n",
    "    def optimize(self, theta, func_params, n_iter):\n",
    "        return self.loss_fn(theta, n_iter, *func_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QuadraticFunction:\n",
    "    def __init__(self, theta=None, W=None, b=None):\n",
    "        self.W = W or T.matrix('W')\n",
    "        self.b = b or T.vector('b')\n",
    "    \n",
    "        self.theta = theta or T.vector('theta')\n",
    "        \n",
    "        self.func = ((T.dot(self.W, self.theta) - self.b)**2).sum()\n",
    "        self.grad = theano.grad(self.func, self.theta)\n",
    "        \n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = T.matrix()\n",
    "b = T.vector()\n",
    "    \n",
    "func = lambda theta: QuadraticFunction(theta, W, b).func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd_step(theta, lr):\n",
    "    f = func(theta)\n",
    "    g = theano.grad(f, theta)\n",
    "    return theta - lr * g, f\n",
    "\n",
    "n_steps = T.iscalar()\n",
    "\n",
    "sgd_lr = T.scalar()\n",
    "sgd_thetas, sgd_losses = theano.scan(fn=sgd_step,\n",
    "                                        outputs_info=[input_var, None],\n",
    "                                                      non_sequences=sgd_lr,\n",
    "                                                      n_steps=n_steps)[0]\n",
    "\n",
    "sgd_fn = theano.function([input_var, n_steps, W, b, sgd_lr], [sgd_thetas, sgd_losses], allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def momentum_step(theta, old_grad, lr, mu):\n",
    "    f = func(theta)\n",
    "    g = theano.grad(f, theta)\n",
    "    new_grad = mu * old_grad + g\n",
    "    return theta - lr * new_grad, f, new_grad\n",
    "\n",
    "momentum_steps = T.iscalar()\n",
    "momentum_mu = T.scalar()\n",
    "\n",
    "momentum_thetas, momentum_losses, _ = theano.scan(fn=momentum_step,\n",
    "                                                  outputs_info=[input_var, None, T.zeros_like(input_var)],\n",
    "                                                  non_sequences=[sgd_lr, momentum_mu],\n",
    "                                                  n_steps=n_steps)[0]\n",
    "\n",
    "momentum_fn = theano.function([input_var, n_steps, W, b, sgd_lr], [momentum_thetas, momentum_losses], allow_input_downcast=True, givens={\n",
    "        momentum_mu: np.cast['float32'](0.9),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm_optimizers = {}\n",
    "\n",
    "options = {\n",
    "    'num_units': 20,\n",
    "    'gradient_steps': 20,\n",
    "    'n_layers': 2,\n",
    "}\n",
    "\n",
    "for n_gac in [0, 5]:\n",
    "    for loss_type in ['sum', 'prod']:\n",
    "        options['n_gac'] = n_gac\n",
    "        options['loss_type'] = loss_type\n",
    "        \n",
    "        key = \"n_gac={}; loss_type={}\".format(n_gac, loss_type)\n",
    "        lstm_optimizers[key] = LSTMOptimizer(input_var, func, [W, b], **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_params(ndim=None):\n",
    "    ndim = ndim or (np.random.randint(low=0, high=50) + 1)\n",
    "    \n",
    "    W = np.cast['float32'](np.random.randn(ndim, ndim))\n",
    "    b = np.cast['float32'](np.random.randn(ndim))\n",
    "    return W, b\n",
    "\n",
    "def sample_point(ndim):\n",
    "    theta = np.cast['float32'](np.random.randn(ndim))\n",
    "    return theta\n",
    "\n",
    "def sample_point_and_params(ndim=None):\n",
    "    W, b = sample_params(ndim)\n",
    "    theta = sample_point(len(b))\n",
    "    \n",
    "    return theta, (W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_options = {\n",
    "    'n_iter': 100,\n",
    "    'n_epochs': 50,\n",
    "    'batch_size': 100,\n",
    "    'decay_rate': 0.96,\n",
    "}\n",
    "\n",
    "for key, opt in lstm_optimizers.items():\n",
    "    opt.train(sample_point_and_params, **training_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_lstm_optimizers = {\n",
    "    'momentum': momentum_fn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_optimizers(**testing_options):\n",
    "    thetas_and_params = [sample_point_and_params() for _ in range(testing_options['n_functions'])]\n",
    "    \n",
    "    for key, opt in lstm_optimizers.items():\n",
    "        print(\"Testing lstm; {key}\".format(**locals()))\n",
    "        loss_history = [] \n",
    "        for theta, (W_, b_) in thetas_and_params:\n",
    "            loss_history.append(opt.optimize(theta, [W_, b_], testing_options['n_iter'])[1])\n",
    "        plt.semilogy(np.median(loss_history, axis=0), label='lstm; {}'.format(key))\n",
    "\n",
    "    lrates = np.logspace(0, 29, num=30, base=2.0) * 1e-6\n",
    "    \n",
    "    for name, opt in non_lstm_optimizers.items():\n",
    "        best_lrate = None\n",
    "        best_loss = None\n",
    "        best_history = None\n",
    "\n",
    "        print(\"Testing {name}\".format(**locals()))\n",
    "        \n",
    "        for lrate in lrates:\n",
    "            loss_history = [] \n",
    "            for theta, (W_, b_) in thetas_and_params:\n",
    "                loss_history.append(opt(theta, testing_options['n_iter'], W_, b_, lrate)[1])\n",
    "            \n",
    "            if np.isnan(loss_history).any():\n",
    "                break\n",
    "            \n",
    "            loss = np.median(loss_history, axis=0)[-1]\n",
    "            if best_loss is None or best_loss > loss:\n",
    "                best_loss = loss\n",
    "                best_lrate = lrate\n",
    "                best_history = np.median(loss_history, axis=0)\n",
    "                \n",
    "        plt.semilogy(best_history, label=\"{name}; lr={best_lrate}\".format(**locals()), linestyle='--')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_options = {\n",
    "    'n_iter': 100,\n",
    "    'n_functions': 50,\n",
    "}\n",
    "\n",
    "test_optimizers(**testing_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta, (W_, b_) = sample_point_and_params(ndim=2)\n",
    "\n",
    "sample_runs = {}\n",
    "\n",
    "for name, opt in lstm_optimizers.items():\n",
    "    history, losses = opt.optimize(theta, [W_, b_], 300)\n",
    "    history = np.concatenate([theta.reshape(1, -1), history], axis=0)\n",
    "    sample_runs[name] = (history, losses)\n",
    "\n",
    "sample_runs['momentum'] = momentum_fn(theta, 300, W_, b_, 0.008192)\n",
    "    \n",
    "# for name, opt in non_lstm_optimizers.items():\n",
    "#     for lrate in np.logspace(0, 29, num=30, base=2.0) * 1e-6:\n",
    "#         if lrate == 0.008192:\n",
    "#             history, losses = opt(theta, 300, W_, b_, lrate)\n",
    "#             history = [theta] + history\n",
    "\n",
    "#             sample_runs[\"{}; lr={}\".format(name, lrate)] = (history, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_opt = np.linalg.pinv(W_).dot(b_)\n",
    "\n",
    "min_x = min(sample_runs[list(lstm_optimizers.keys())[0]][0].T[0])\n",
    "max_x = max(sample_runs[list(lstm_optimizers.keys())[0]][0].T[0])\n",
    "min_y = min(sample_runs[list(lstm_optimizers.keys())[0]][0].T[1])\n",
    "max_y = max(sample_runs[list(lstm_optimizers.keys())[0]][0].T[1])\n",
    "\n",
    "delta_x = (max_x - min_x) / 100.\n",
    "delta_y = (max_y - min_y) / 100.\n",
    "\n",
    "x = np.arange(2 * min_x - (min_x + max_x) / 2, 2 * max_x - (min_x + max_x) / 2, delta_x)\n",
    "y = np.arange(2 * min_y - (min_y + max_y) / 2, 2 * max_y - (min_y + max_y) / 2, delta_y)\n",
    "# x = np.arange(min_x, max_x, delta_x)\n",
    "# y = np.arange(min_y, max_y, delta_y)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = np.zeros(X.shape)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        z = ((W_.dot(np.array([X[i][j], Y[i][j]])) - b_)**2).sum()\n",
    "        Z[i][j] = z\n",
    "\n",
    "#Z = ((W.dot(X) - b_)**2).sum()\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.title('Trajectory')\n",
    "\n",
    "CS = plt.contour(X, Y, Z, levels=[1e-4, 1e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1e-0, 5e-0, 1e1])\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "\n",
    "for name in lstm_optimizers:\n",
    "    history, _ = sample_runs[name]\n",
    "    plt.plot(history.T[0], history.T[1], marker='o', label=name)\n",
    "\n",
    "history, _ = sample_runs['momentum']\n",
    "plt.plot(np.array(history).T[0], np.array(history).T[1], label='momentum', marker='o', linestyle='--')\n",
    "\n",
    "# for name in non_lstm_optimizers:\n",
    "#     for lrate in np.logspace(0, 29, num=30, base=2.0) * 1e-6:\n",
    "#         if lrate == 0.008192:\n",
    "#         #if 1e-4 < lrate < 1e-1 and name != 'sgd':\n",
    "#             key = \"{}; lr={}\".format(name, lrate)\n",
    "#             history, _ = sample_runs[key]\n",
    "#             plt.plot(np.array(history).T[0], np.array(history).T[1], label=key, marker='o', linestyle='--')\n",
    "\n",
    "print(theta_opt)\n",
    "plt.plot([theta_opt[0]], [theta_opt[1]], marker='x', color='k')            \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "plt.title('loss/step')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "for name in lstm_optimizers:\n",
    "    _, losses = sample_runs[name]\n",
    "    plt.semilogy(losses, label=name)\n",
    "\n",
    "_, losses = sample_runs['momentum']    \n",
    "plt.semilogy(losses, label='momentum', linestyle='--')\n",
    "\n",
    "# for name in non_lstm_optimizers:\n",
    "#     for lrate in np.logspace(0, 29, num=30, base=2.0) * 1e-6:\n",
    "#         if lrate == 0.008912:\n",
    "#         #if lrate < 1e-1 and name != 'sgd':\n",
    "#             key = \"{}; lr={}\".format(name, lrate)\n",
    "#             _, losses = sample_runs[key]\n",
    "#             plt.semilogy(losses, label=key, linestyle='--')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generalization_loss_lstm = {name: [] for name in lstm_optimizers}\n",
    "\n",
    "for n_c in range(50, 75):\n",
    "    print(n_c)\n",
    "    for name, opt in lstm_optimizers.items():\n",
    "        losses = []\n",
    "        \n",
    "        for _ in range(10):\n",
    "            theta, (W_, b_) = sample_point_and_params(ndim=n_c)    \n",
    "            loss = 1. / n_c * opt.optimize(theta, [W_, b_], 100)[1][-1]\n",
    "            losses.append(loss)\n",
    "    \n",
    "        generalization_loss_lstm[name].append(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, losses in generalization_loss_lstm.items():\n",
    "    plt.semilogy(range(2, 75), losses, label=name)\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, opt in lstm_optimizers.items():\n",
    "    np.savez('optimizer_({}).npz'.format(name), L.layers.get_all_param_values(opt.l_optim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_options = {\n",
    "    'n_iter': 100,\n",
    "    'n_functions': 50,\n",
    "}\n",
    "\n",
    "test_optimizers(**testing_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_options = {\n",
    "    'n_iter': 300,\n",
    "    'n_functions': 50,\n",
    "}\n",
    "\n",
    "test_optimizers(**testing_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
