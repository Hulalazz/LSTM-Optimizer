{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla M2090 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne as L\n",
    "\n",
    "from theano.printing import Print as TPP\n",
    "\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../lstm_optimizer_drop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMOptimizer:\n",
    "    def __init__(self, input_var, func, func_params, loss_type='sum', lambd=1e-5, **kwargs):\n",
    "        n_steps = T.iscalar()\n",
    "        \n",
    "        l_input = L.layers.InputLayer(shape=(None,), input_var=input_var)\n",
    "        self.l_optim = LSTMOptimizerLayer(l_input, n_steps=n_steps, function=func, **kwargs)\n",
    "        \n",
    "        self.params_init = L.layers.get_all_param_values(self.l_optim)\n",
    "\n",
    "        theta_history, loss_history, scan_updates = L.layers.get_output(self.l_optim)\n",
    "\n",
    "        self.loss_type = loss_type\n",
    "        if loss_type == 'sum':\n",
    "            loss = loss_history.sum()\n",
    "        elif loss_type == 'prod':\n",
    "            loss = T.log(loss_history).sum()\n",
    "        elif loss_type == 'weighted_prod':\n",
    "            loss = (T.log(loss_history) * 0.9**T.arange(loss_history.shape[0])[::-1]).sum()\n",
    "        elif loss_type == 'norm_sum':\n",
    "            loss = loss_history[1:].sum() / loss_history[0]\n",
    "        elif loss_type == 'rel_sum':\n",
    "            loss = (loss_history[1:] / loss_history[:-1]).sum()\n",
    "                \n",
    "        loss += lambd * L.regularization.regularize_network_params(self.l_optim, L.regularization.l2)\n",
    "                \n",
    "        self.lr = theano.shared(np.array(0.01, dtype=np.float32))\n",
    "\n",
    "        params = L.layers.get_all_params(self.l_optim)\n",
    "        updates = L.updates.adam(loss, params, learning_rate=self.lr)\n",
    "        updates.update(scan_updates)\n",
    "        \n",
    "        self.loss_fn = theano.function([input_var, n_steps] + func_params, [theta_history, loss_history], allow_input_downcast=True, updates=scan_updates)\n",
    "        self.train_fn = theano.function([input_var, n_steps] + func_params, [theta_history, loss_history], updates=updates, allow_input_downcast=True)\n",
    "        \n",
    "        theta_history_det, loss_history_det, scan_updates_det = L.layers.get_output(self.l_optim, deterministic=True)\n",
    "        self.loss_det_fn = theano.function([input_var, n_steps] + func_params, [theta_history_det, loss_history_det], allow_input_downcast=True, updates=scan_updates_det)\n",
    "        \n",
    "    def reset_network(self):\n",
    "        L.layers.set_all_param_values(self.l_optim, self.params_init)\n",
    "        \n",
    "    def train(self, sample_function, n_iter=100, n_epochs=50, batch_size=100, decay_rate=0.96):\n",
    "        optimizer_loss = []\n",
    "        optimizer_moving_loss = []\n",
    "        moving_loss = None\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "            t = time.time()    \n",
    "\n",
    "            training_loss_history = []\n",
    "            for j in range(batch_size):\n",
    "                theta, params = sample_function()\n",
    " \n",
    "                theta_history, loss_history = self.train_fn(theta, n_iter, *params)\n",
    "                if self.loss_type == 'sum':\n",
    "                    loss = np.sum(loss_history)\n",
    "                else:\n",
    "                    loss = np.sum(np.log(loss_history))\n",
    "                training_loss_history.append(loss_history)\n",
    "            \n",
    "                optimizer_loss.append(loss)\n",
    "                if moving_loss is None:\n",
    "                    moving_loss = loss\n",
    "                else:\n",
    "                    moving_loss = 0.9 * moving_loss + 0.1 * loss\n",
    "                optimizer_moving_loss.append(moving_loss)\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "            print(\"Epoch number {}\".format(i))\n",
    "            t = time.time() - t\n",
    "\n",
    "            print(\"Time on epoch {}: {}\".format(i, t))\n",
    "            print(\"Loss on {}: {}; {}\".format(i, loss, np.median(training_loss_history, axis=0)[-1]))\n",
    "\n",
    "            self.lr.set_value((self.lr.get_value() * decay_rate).astype(np.float32))\n",
    "            \n",
    "    def optimize(self, theta, func_params, n_iter):\n",
    "        return self.loss_fn(theta, n_iter, *func_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QuadraticFunction:\n",
    "    def __init__(self, theta=None, W=None, b=None):\n",
    "        self.W = W or T.matrix('W')\n",
    "        self.b = b or T.vector('b')\n",
    "    \n",
    "        self.theta = theta or T.vector('theta')\n",
    "        \n",
    "        self.func = ((T.dot(self.W, self.theta) - self.b)**2).sum() / self.W.shape[0]\n",
    "        self.grad = theano.grad(self.func, self.theta)\n",
    "        \n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = T.matrix()\n",
    "b = T.vector()\n",
    "    \n",
    "func = lambda theta: QuadraticFunction(theta, W, b).func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd_step(theta, lr):\n",
    "    f = func(theta)\n",
    "    g = theano.grad(f, theta)\n",
    "    return theta - lr * g, f\n",
    "\n",
    "n_steps = T.iscalar()\n",
    "\n",
    "sgd_lr = T.scalar()\n",
    "sgd_thetas, sgd_losses = theano.scan(fn=sgd_step,\n",
    "                                        outputs_info=[input_var, None],\n",
    "                                                      non_sequences=sgd_lr,\n",
    "                                                      n_steps=n_steps)[0]\n",
    "\n",
    "sgd_fn = theano.function([input_var, n_steps, W, b, sgd_lr], [sgd_thetas, sgd_losses], allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def momentum_step(theta, old_grad, lr, mu):\n",
    "    f = func(theta)\n",
    "    g = theano.grad(f, theta)\n",
    "    new_grad = mu * old_grad + g\n",
    "    return theta - lr * new_grad, f, new_grad\n",
    "\n",
    "momentum_steps = T.iscalar()\n",
    "momentum_mu = T.scalar()\n",
    "\n",
    "momentum_thetas, momentum_losses, _ = theano.scan(fn=momentum_step,\n",
    "                                                  outputs_info=[input_var, None, T.zeros_like(input_var)],\n",
    "                                                  non_sequences=[sgd_lr, momentum_mu],\n",
    "                                                  n_steps=n_steps)[0]\n",
    "\n",
    "momentum_fn = theano.function([input_var, n_steps, W, b, sgd_lr], [momentum_thetas, momentum_losses], allow_input_downcast=True, givens={\n",
    "        momentum_mu: np.cast['float32'](0.9),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gac=0; p_drop_coord=0.0; fix=True\n",
      "n_gac=0; p_drop_coord=0.5; fix=False\n",
      "n_gac=0; p_drop_coord=0.5; fix=True\n",
      "n_gac=5; p_drop_coord=0.0; fix=True\n",
      "n_gac=5; p_drop_coord=0.5; fix=False\n",
      "n_gac=5; p_drop_coord=0.5; fix=True\n"
     ]
    }
   ],
   "source": [
    "lstm_optimizers = {}\n",
    "\n",
    "options = {\n",
    "    'num_units': 20,\n",
    "    'gradient_steps': 20,\n",
    "    'n_layers': 2,\n",
    "    'preprocess_input': False,\n",
    "    \n",
    "    'p_drop_grad': 0.0,\n",
    "    'p_drop_delta': 0.0,\n",
    "    'fix_drop_delta_over_time': True,\n",
    "    'fix_drop_grad_over_time': True,\n",
    "    \n",
    "    'loss_type': 'sum',\n",
    "}\n",
    "\n",
    "for n_gac in [0, 5]:\n",
    "    for p_drop_coord in [0.0, 0.5]:\n",
    "        for fix in [False, True]:\n",
    "            if p_drop_coord == 0.0 and not fix:\n",
    "                continue\n",
    "            \n",
    "            options['n_gac'] = n_gac\n",
    "            options['p_drop_coord'] = p_drop_coord\n",
    "            options['fix_drop_coord_over_time'] = fix\n",
    "\n",
    "            key = \"n_gac={}; p_drop_coord={}; fix={}\".format(n_gac, p_drop_coord, fix)\n",
    "            if options.get(key, None) is None:\n",
    "                lstm_optimizers[key] = LSTMOptimizer(input_var, func, [W, b], **options)\n",
    "                print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_params(ndim=None):\n",
    "    ndim = ndim or (np.random.randint(low=0, high=1000) + 1)\n",
    "    \n",
    "    W = np.cast['float32'](np.random.randn(ndim, ndim))\n",
    "    b = np.cast['float32'](np.random.randn(ndim))\n",
    "    return W, b\n",
    "\n",
    "def sample_point(ndim):\n",
    "    theta = np.cast['float32'](np.random.randn(ndim))\n",
    "    return theta\n",
    "\n",
    "def sample_point_and_params(ndim=None):\n",
    "    W, b = sample_params(ndim)\n",
    "    theta = sample_point(len(b))\n",
    "    \n",
    "    return theta, (W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 0\n",
      "Time on epoch 0: 9.985090732574463\n",
      "Loss on 0: 277.8734559416771; 0.7874714732170105\n"
     ]
    }
   ],
   "source": [
    "training_options = {\n",
    "    'n_iter': 100,\n",
    "    'n_epochs': 50,\n",
    "    'batch_size': 300,\n",
    "    'decay_rate': 0.96,\n",
    "}\n",
    "\n",
    "for key, opt in lstm_optimizers.items():\n",
    "    opt.reset_network()\n",
    "    opt.lr.set_value(0.01)\n",
    "\n",
    "    if key.find('p_drop_coord=0.0') != -1:\n",
    "        sampler = sample_point_and_params\n",
    "    else:\n",
    "        sampler = lambda: sample_point_and_params(ndim=100)\n",
    "    \n",
    "    for n_iter in [20, 40, 60]:\n",
    "        training_options['n_iter'] = n_iter\n",
    "        opt.train(sampler, **training_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_lstm_optimizers = {\n",
    "    'momentum': momentum_fn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for name, opt in lstm_optimizers.items():\n",
    "#     with open('quadratic_optimizer_drop_coord_low_dim({}).npz'.format(name), 'wb') as f:\n",
    "#         np.savez(f, L.layers.get_all_param_values(opt.l_optim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for name, opt in lstm_optimizers.items():\n",
    "#     with np.load('quadratic_optimizer_drop_coord_low_dim({}).npz'.format(name)) as f:\n",
    "#         param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "#     L.layers.set_all_param_values(opt.l_optim, param_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "generalization_loss_lstm = {}\n",
    "for name in lstm_optimizers:\n",
    "    generalization_loss_lstm[name] = []\n",
    "    generalization_loss_lstm[name + \"_det\"] = []\n",
    "\n",
    "for n_c in itertools.chain(range(2, 150), range(400, 500), range(900, 1000), range(10000, 10100)):\n",
    "    print(n_c)\n",
    "\n",
    "    points_and_params = [sample_point_and_params(ndim=n_c) for _ in range(10)]\n",
    "    \n",
    "    for name, opt in lstm_optimizers.items():\n",
    "        losses = []\n",
    "        losses_det = []\n",
    "        \n",
    "        for theta, (W_, b_) in points_and_params:\n",
    "            loss = opt.optimize(theta, [W_, b_], 100)[1][-1]\n",
    "            loss_det = opt.loss_det_fn(theta, 100, W_, b_)[1][-1]\n",
    "            \n",
    "            losses.append(loss)\n",
    "            losses_det.append(loss_det)\n",
    "    \n",
    "        generalization_loss_lstm[name].append(np.mean(losses))\n",
    "        generalization_loss_lstm[name + \"_det\"].append(np.mean(losses_det))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 24))\n",
    "\n",
    "for name, losses in sorted(generalization_loss_lstm.items()): \n",
    "    if name.find('0.0') != -1 and name.find('det') != -1:\n",
    "        continue\n",
    "    if name.find('det') == -1:\n",
    "        linestyle = 'solid'\n",
    "    elif name.find('fix=False') != -1:\n",
    "        linestyle = '-.'\n",
    "    else:\n",
    "        linestyle = '--'\n",
    "        \n",
    "    ax1.semilogy(list(itertools.chain(range(2, 150), range(400, 500), range(900, 1000), range(10000, 10100))), losses[:], label=name, linestyle=linestyle)\n",
    "    ax2.semilogy(list(range(10000, 10100)), losses[-100:], label=name, linestyle=linestyle)\n",
    "    ax3.semilogy(list(itertools.chain(range(22, 150), range(400, 500), range(900, 1000), range(10000, 10100))), losses[20:], label=name, linestyle=linestyle)\n",
    "    \n",
    "# fig.legend(loc=4);\n",
    "ax1.legend(loc=4)\n",
    "ax2.legend(loc=4)\n",
    "ax3.legend(loc=2)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_optimizers(**testing_options):\n",
    "    thetas_and_params = [testing_options['sampler']() for _ in range(testing_options['n_functions'])]\n",
    "    \n",
    "    histories = {}\n",
    "    \n",
    "    for key, opt in lstm_optimizers.items():\n",
    "        print(\"Testing lstm; {key}\".format(**locals()))\n",
    "        loss_history = [] \n",
    "        loss_history_det = [] \n",
    "        for theta, (W_, b_) in thetas_and_params:\n",
    "            loss_history.append(opt.optimize(theta, [W_, b_], testing_options['n_iter'])[1])\n",
    "            loss_history_det.append(opt.loss_det_fn(theta, testing_options['n_iter'], W_, b_)[1])\n",
    "            \n",
    "        histories['lstm; {}'.format(key)] = np.median(loss_history, axis=0)\n",
    "        histories['lstm_det; {}'.format(key)] = np.median(loss_history_det, axis=0)\n",
    "\n",
    "    lrates = np.logspace(0, 29, num=30, base=2.0) * 1e-6\n",
    "    \n",
    "    for name, opt in non_lstm_optimizers.items():\n",
    "        best_lrate = None\n",
    "        best_loss = None\n",
    "        best_history = None\n",
    "\n",
    "        print(\"Testing {name}\".format(**locals()))\n",
    "        \n",
    "        for lrate in lrates:\n",
    "            loss_history = [] \n",
    "            for theta, (W_, b_) in thetas_and_params:\n",
    "                loss_history.append(opt(theta, testing_options['n_iter'], W_, b_, lrate)[1])\n",
    "            \n",
    "            if np.isnan(loss_history).any():\n",
    "                break\n",
    "            \n",
    "            loss = np.median(loss_history, axis=0)[-1]\n",
    "            if best_loss is None or best_loss > loss:\n",
    "                best_loss = loss\n",
    "                best_lrate = lrate\n",
    "                best_history = np.median(loss_history, axis=0)\n",
    "                \n",
    "        histories[\"{name}; lr={best_lrate}\".format(**locals())] = best_history\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_options = {\n",
    "    'n_iter': 60,\n",
    "    'n_functions': 50,\n",
    "    'sampler': sample_point_and_params\n",
    "}\n",
    "\n",
    "histories_60 = test_optimizers(**testing_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_options = {\n",
    "    'n_iter': 500,\n",
    "    'n_functions': 50,\n",
    "    'sampler': sample_point_and_params\n",
    "}\n",
    "\n",
    "histories_500 = test_optimizers(**testing_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_options = {\n",
    "    'n_iter': 500,\n",
    "    'n_functions': 1,\n",
    "    'sampler': sample_point_and_params\n",
    "}\n",
    "\n",
    "histories_1_func = test_optimizers(**testing_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 18))\n",
    "\n",
    "for name in sorted(set(list(histories_60.keys()) + list(histories_500.keys()) + list(histories_1_func.keys()))):\n",
    "    if name.find('0.0') != -1 and name.find('det') != -1:\n",
    "        continue\n",
    "    if name.find('det') == -1:\n",
    "        linestyle = 'solid'\n",
    "    elif name.find('fix=False') != -1:\n",
    "        linestyle = '-.'\n",
    "    else:\n",
    "        linestyle = '--'\n",
    "        \n",
    "    if histories_60.get(name) is not None:\n",
    "        ax1.semilogy(histories_60[name], label=name, linestyle=linestyle)\n",
    "    if histories_500.get(name) is not None:\n",
    "        ax2.semilogy(histories_500[name], label=name, linestyle=linestyle)\n",
    "    if histories_1_func.get(name) is not None:\n",
    "        ax3.semilogy(histories_1_func[name], label=name, linestyle=linestyle)\n",
    "    \n",
    "# fig.legend(loc=4);\n",
    "ax1.set_title('50 functions; 60 iterations')\n",
    "ax2.set_title('50 functions; 500 iterations')\n",
    "ax3.set_title('1 function; 500 iterations')\n",
    "ax1.legend(loc=4)\n",
    "ax2.legend(loc=4)\n",
    "ax3.legend(loc=4)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta, (W_, b_) = sample_point_and_params(ndim=2)\n",
    "\n",
    "sample_runs = {}\n",
    "\n",
    "for name, opt in lstm_optimizers.items():\n",
    "    history, losses = opt.optimize(theta, [W_, b_], 100)\n",
    "    history = np.concatenate([theta.reshape(1, -1), history], axis=0)\n",
    "    sample_runs[name] = (history, losses)\n",
    "    \n",
    "    history, losses = opt.loss_det_fn(theta, 100, W_, b_)\n",
    "    history = np.concatenate([theta.reshape(1, -1), history], axis=0)\n",
    "    sample_runs[name+'_det'] = (history, losses)\n",
    "\n",
    "\n",
    "sample_runs['momentum'] = momentum_fn(theta, 100, W_, b_, 0.262144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_opt = np.linalg.pinv(W_).dot(b_)\n",
    "\n",
    "min_x = min(sample_runs[list(lstm_optimizers.keys())[1]][0].T[0])\n",
    "max_x = max(sample_runs[list(lstm_optimizers.keys())[1]][0].T[0])\n",
    "min_y = min(sample_runs[list(lstm_optimizers.keys())[1]][0].T[1])\n",
    "max_y = max(sample_runs[list(lstm_optimizers.keys())[1]][0].T[1])\n",
    "\n",
    "delta_x = (max_x - min_x) / 100.\n",
    "delta_y = (max_y - min_y) / 100.\n",
    "\n",
    "x = np.arange(2 * min_x - (min_x + max_x) / 2, 2 * max_x - (min_x + max_x) / 2, delta_x)\n",
    "y = np.arange(2 * min_y - (min_y + max_y) / 2, 2 * max_y - (min_y + max_y) / 2, delta_y)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = np.zeros(X.shape)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        z = ((W_.dot(np.array([X[i][j], Y[i][j]])) - b_)**2).sum()\n",
    "        Z[i][j] = z\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.title('Trajectory')\n",
    "\n",
    "CS = plt.contour(X, Y, Z, levels=[1e-4, 1e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1e-0, 5e-0, 1e1])\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "\n",
    "for name in sample_runs:\n",
    "    history, lss = sample_runs[name]\n",
    "    if lss[-1] > 0.1 or name == 'momentum':#or name.find('fix=False') == -1:\n",
    "        continue\n",
    "    if name.find('0.0') != -1 and name.find('det') != -1:\n",
    "        continue\n",
    "    if name.find('det') == -1:\n",
    "        linestyle = 'solid'\n",
    "    elif name.find('fix=False') != -1:\n",
    "        linestyle = '-.'\n",
    "    else:\n",
    "        linestyle = '--'\n",
    "    plt.plot(history.T[0], history.T[1], linestyle=linestyle, label=name, marker='x')\n",
    "\n",
    "# history, _ = sample_runs['momentum']\n",
    "# plt.plot(np.array(history).T[0], np.array(history).T[1], label='momentum', marker='o', linestyle='--')\n",
    "\n",
    "print(theta_opt)\n",
    "plt.plot([theta_opt[0]], [theta_opt[1]], marker='x', color='k')            \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "plt.title('loss/step')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "for name in sample_runs:\n",
    "    _, losses = sample_runs[name]\n",
    "    if losses[-1] > 0.1:#or name.find('fix=False') == -1:\n",
    "        continue\n",
    "\n",
    "    if name.find('0.0') != -1 and name.find('det') != -1:\n",
    "        continue\n",
    "    if name.find('det') == -1:\n",
    "        linestyle = 'solid'\n",
    "    elif name.find('fix=False') != -1:\n",
    "        linestyle = '-.'\n",
    "    else:\n",
    "        linestyle = '--'\n",
    "        \n",
    "    plt.semilogy(losses, label=name, linestyle=linestyle)\n",
    "\n",
    "# _, losses = sample_runs['momentum']    \n",
    "# plt.semilogy(losses, label='momentum', linestyle='--')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('gen_loss_8', 'wb') as f:\n",
    "    pickle.dump(generalization_loss_lstm, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 24))\n",
    "\n",
    "for name, losses in sorted(generalization_loss_lstm.items()): \n",
    "    if name.find('0.0') != -1 and name.find('det') != -1:\n",
    "        continue\n",
    "    if name.find('fix=False') != -1:\n",
    "        linestyle = '-.'\n",
    "    elif name.find('det') == -1:\n",
    "        linestyle = 'solid'\n",
    "    else:\n",
    "        linestyle = '--'\n",
    "        \n",
    "    ax1.semilogy(list(itertools.chain(range(2, 150), range(400, 500), range(900, 1000), range(10000, 10100))), losses[:], label=name, linestyle=linestyle)\n",
    "    ax2.semilogy(list(range(10000, 10100)), losses[-100:], label=name, linestyle=linestyle)\n",
    "    ax3.semilogy(list(itertools.chain(range(22, 150), range(400, 500), range(900, 1000), range(10000, 10100))), losses[20:], label=name, linestyle=linestyle)\n",
    "    \n",
    "# fig.legend(loc=4);\n",
    "ax1.legend(loc=4)\n",
    "ax2.legend(loc=4)\n",
    "ax3.legend(loc=2)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
