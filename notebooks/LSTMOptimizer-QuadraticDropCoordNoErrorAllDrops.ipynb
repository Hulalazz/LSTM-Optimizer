{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla M2090 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne as L\n",
    "\n",
    "from theano.printing import Print as TPP\n",
    "\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../lstm_optimizer_drop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMOptimizer:\n",
    "    def __init__(self, input_var, func, func_params, loss_type='sum', lambd=1e-5, **kwargs):\n",
    "        n_steps = T.iscalar()\n",
    "        \n",
    "        l_input = L.layers.InputLayer(shape=(None,), input_var=input_var)\n",
    "        self.l_optim = LSTMOptimizerLayer(l_input, n_steps=n_steps, function=func, **kwargs)\n",
    "        \n",
    "        self.params_init = L.layers.get_all_param_values(self.l_optim)\n",
    "\n",
    "        theta_history, loss_history, scan_updates = L.layers.get_output(self.l_optim)\n",
    "\n",
    "        self.loss_type = loss_type\n",
    "        if loss_type == 'sum':\n",
    "            loss = loss_history.sum()\n",
    "        elif loss_type == 'prod':\n",
    "            loss = T.log(loss_history).sum()\n",
    "        elif loss_type == 'weighted_prod':\n",
    "            loss = (T.log(loss_history) * 0.9**T.arange(loss_history.shape[0])[::-1]).sum()\n",
    "                \n",
    "        loss += lambd * L.regularization.regularize_network_params(self.l_optim, L.regularization.l2)\n",
    "                \n",
    "        self.lr = theano.shared(np.array(0.01, dtype=np.float32))\n",
    "\n",
    "        params = L.layers.get_all_params(self.l_optim)\n",
    "        updates = L.updates.adam(loss, params, learning_rate=self.lr)\n",
    "        updates.update(scan_updates)\n",
    "        \n",
    "        self.loss_fn = theano.function([input_var, n_steps] + func_params, [theta_history, loss_history], allow_input_downcast=True, updates=scan_updates)\n",
    "        self.train_fn = theano.function([input_var, n_steps] + func_params, [theta_history, loss_history], updates=updates, allow_input_downcast=True)\n",
    "        \n",
    "        theta_history_det, loss_history_det, scan_updates_det = L.layers.get_output(self.l_optim, deterministic=True)\n",
    "        self.loss_det_fn = theano.function([input_var, n_steps] + func_params, [theta_history_det, loss_history_det], allow_input_downcast=True, updates=scan_updates_det)\n",
    "        \n",
    "    def reset_network(self):\n",
    "        L.layers.set_all_param_values(self.l_optim, self.params_init)\n",
    "        \n",
    "    def train(self, sample_function, n_iter=100, n_epochs=50, batch_size=100, decay_rate=0.96):\n",
    "        optimizer_loss = []\n",
    "        optimizer_moving_loss = []\n",
    "        moving_loss = None\n",
    "\n",
    "#         fig = plt.figure(figsize=(15, 12))\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            t = time.time()    \n",
    "\n",
    "            training_loss_history = []\n",
    "            for j in range(batch_size):\n",
    "                theta, params = sample_function()\n",
    " \n",
    "                theta_history, loss_history = self.train_fn(theta, n_iter, *params)\n",
    "                if self.loss_type == 'sum':\n",
    "                    loss = np.sum(loss_history)\n",
    "                else:\n",
    "                    loss = np.sum(np.log(loss_history))\n",
    "                training_loss_history.append(loss_history)\n",
    "            \n",
    "                optimizer_loss.append(loss)\n",
    "                if moving_loss is None:\n",
    "                    moving_loss = loss\n",
    "                else:\n",
    "                    moving_loss = 0.9 * moving_loss + 0.1 * loss\n",
    "                optimizer_moving_loss.append(moving_loss)\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "#             fig.clear()\n",
    "            \n",
    "#             ax = fig.add_subplot(2, 1, 2)\n",
    "#             ax.semilogy(np.median(training_loss_history, axis=0))\n",
    "#             #plt.subplot(2, 1, 2)\n",
    "            #plt.semilogy(np.median(training_loss_history, axis=0))\n",
    "\n",
    "#             ax = fig.add_subplot(2, 1, 1)\n",
    "            #plt.subplot(2, 1, 1)\n",
    "#             if self.loss_type == 'sum':\n",
    "#                 ax.semilogy(optimizer_loss, 'b', label='loss')\n",
    "#                 ax.semilogy(optimizer_moving_loss, 'r', label='moving loss')\n",
    "#                 #plt.semilogy(optimizer_loss, 'b', label='loss')\n",
    "#                 #plt.semilogy(optimizer_moving_loss, 'r', label='moving loss')\n",
    "#             else:\n",
    "#                 ax.plot(optimizer_loss, 'b', label='loss')\n",
    "#                 ax.plot(optimizer_moving_loss, 'r', label='moving loss')\n",
    "#                 #plt.plot(optimizer_loss, 'b', label='loss')\n",
    "#                 #plt.plot(optimizer_moving_loss, 'r', label='moving loss')\n",
    "\n",
    "#             fig.canvas.draw()\n",
    "#             #plt.show()\n",
    "            print(\"Epoch number {}\".format(i))\n",
    "            t = time.time() - t\n",
    "\n",
    "            print(\"Time on epoch {}: {}\".format(i, t))\n",
    "            print(\"Loss on {}: {}; {}\".format(i, loss, np.median(training_loss_history, axis=0)[-1]))\n",
    "\n",
    "            self.lr.set_value((self.lr.get_value() * decay_rate).astype(np.float32))\n",
    "            \n",
    "    def optimize(self, theta, func_params, n_iter):\n",
    "        return self.loss_fn(theta, n_iter, *func_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QuadraticFunction:\n",
    "    def __init__(self, theta=None, W=None, b=None):\n",
    "        self.W = W or T.matrix('W')\n",
    "        self.b = b or T.vector('b')\n",
    "    \n",
    "        self.theta = theta or T.vector('theta')\n",
    "        \n",
    "        self.func = ((T.dot(self.W, self.theta) - self.b)**2).sum() / self.W.shape[0]\n",
    "        self.grad = theano.grad(self.func, self.theta)\n",
    "        \n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DiagFunction:\n",
    "    def __init__(self, theta=None, W=None, b=None):\n",
    "        self.W = W or T.vector('W')\n",
    "        self.b = b or T.vector('b')\n",
    "    \n",
    "        self.theta = theta or T.vector('theta')\n",
    "        \n",
    "        self.func = ((self.W * self.theta - self.b)**2).sum() / self.W.shape[0]\n",
    "        self.grad = theano.grad(self.func, self.theta)\n",
    "        \n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = T.matrix()\n",
    "b = T.vector()\n",
    "    \n",
    "w = T.vector()\n",
    "    \n",
    "func = lambda theta: QuadraticFunction(theta, W, b).func\n",
    "# func = lambda theta: DiagFunction(theta, w, b).func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_var = T.vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd_step(theta, lr):\n",
    "    f = func(theta)\n",
    "    g = theano.grad(f, theta)\n",
    "    return theta - lr * g, f\n",
    "\n",
    "n_steps = T.iscalar()\n",
    "\n",
    "sgd_lr = T.scalar()\n",
    "sgd_thetas, sgd_losses = theano.scan(fn=sgd_step,\n",
    "                                        outputs_info=[input_var, None],\n",
    "                                                      non_sequences=sgd_lr,\n",
    "                                                      n_steps=n_steps)[0]\n",
    "\n",
    "sgd_fn = theano.function([input_var, n_steps, W, b, sgd_lr], [sgd_thetas, sgd_losses], allow_input_downcast=True)\n",
    "# sgd_fn = theano.function([input_var, n_steps, w, b, sgd_lr], [sgd_thetas, sgd_losses], allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def momentum_step(theta, old_grad, lr, mu):\n",
    "    f = func(theta)\n",
    "    g = theano.grad(f, theta)\n",
    "    new_grad = mu * old_grad + g\n",
    "    return theta - lr * new_grad, f, new_grad\n",
    "\n",
    "momentum_steps = T.iscalar()\n",
    "momentum_mu = T.scalar()\n",
    "\n",
    "momentum_thetas, momentum_losses, _ = theano.scan(fn=momentum_step,\n",
    "                                                  outputs_info=[input_var, None, T.zeros_like(input_var)],\n",
    "                                                  non_sequences=[sgd_lr, momentum_mu],\n",
    "                                                  n_steps=n_steps)[0]\n",
    "\n",
    "momentum_fn = theano.function([input_var, n_steps, W, b, sgd_lr], [momentum_thetas, momentum_losses], allow_input_downcast=True, givens={\n",
    "        momentum_mu: np.cast['float32'](0.9),\n",
    "    })\n",
    "\n",
    "# momentum_fn = theano.function([input_var, n_steps, w, b, sgd_lr], [momentum_thetas, momentum_losses], allow_input_downcast=True, givens={\n",
    "#         momentum_mu: np.cast['float32'](0.9),\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_gac=0; p_drop_(coord,grad,delta)=(0.0, 0.0, 0.0), fix_(coord,grad,delta)=(True, True, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.0, 0.5, 0.0), fix_(coord,grad,delta)=(True, False, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.0, 0.5, 0.0), fix_(coord,grad,delta)=(True, True, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.0, 0.0, 0.5), fix_(coord,grad,delta)=(True, True, False)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.0, 0.0, 0.5), fix_(coord,grad,delta)=(True, True, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.0, 0.5, 0.5), fix_(coord,grad,delta)=(True, False, False)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.0, 0.5, 0.5), fix_(coord,grad,delta)=(True, True, False)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.0, 0.5, 0.5), fix_(coord,grad,delta)=(True, False, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.0, 0.5, 0.5), fix_(coord,grad,delta)=(True, True, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.0, 0.0), fix_(coord,grad,delta)=(False, True, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.0, 0.0), fix_(coord,grad,delta)=(True, True, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.5, 0.0), fix_(coord,grad,delta)=(False, False, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.5, 0.0), fix_(coord,grad,delta)=(False, True, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.5, 0.0), fix_(coord,grad,delta)=(True, False, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.5, 0.0), fix_(coord,grad,delta)=(True, True, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.0, 0.5), fix_(coord,grad,delta)=(False, True, False)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.0, 0.5), fix_(coord,grad,delta)=(False, True, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.0, 0.5), fix_(coord,grad,delta)=(True, True, False)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.0, 0.5), fix_(coord,grad,delta)=(True, True, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.5, 0.5), fix_(coord,grad,delta)=(False, False, False)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.5, 0.5), fix_(coord,grad,delta)=(False, True, False)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.5, 0.5), fix_(coord,grad,delta)=(False, False, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.5, 0.5), fix_(coord,grad,delta)=(False, True, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.5, 0.5), fix_(coord,grad,delta)=(True, False, False)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.5, 0.5), fix_(coord,grad,delta)=(True, True, False)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.5, 0.5), fix_(coord,grad,delta)=(True, False, True)\n",
      "n_gac=0; p_drop_(coord,grad,delta)=(0.5, 0.5, 0.5), fix_(coord,grad,delta)=(True, True, True)\n",
      "n_gac=5; p_drop_(coord,grad,delta)=(0.0, 0.0, 0.0), fix_(coord,grad,delta)=(True, True, True)\n",
      "n_gac=5; p_drop_(coord,grad,delta)=(0.0, 0.5, 0.0), fix_(coord,grad,delta)=(True, False, True)\n",
      "n_gac=5; p_drop_(coord,grad,delta)=(0.0, 0.5, 0.0), fix_(coord,grad,delta)=(True, True, True)\n",
      "n_gac=5; p_drop_(coord,grad,delta)=(0.0, 0.0, 0.5), fix_(coord,grad,delta)=(True, True, False)\n"
     ]
    }
   ],
   "source": [
    "lstm_optimizers = {}\n",
    "\n",
    "options = {\n",
    "    'num_units': 20,\n",
    "    'gradient_steps': 20,\n",
    "    'n_layers': 2,\n",
    "    'preprocess_input': False,\n",
    "    'loss_type': 'sum',\n",
    "}\n",
    "\n",
    "for n_gac in [0, 5]:\n",
    "    for p_drop_coord in [0.0, 0.5]:\n",
    "        for p_drop_delta in [0.0, 0.5]:\n",
    "            for p_drop_grad in [0.0, 0.5]:\n",
    "                for fix_coord in [False, True]:\n",
    "                    for fix_delta in [False, True]:\n",
    "                        for fix_grad in [False, True]:\n",
    "                            if p_drop_coord == 0.0 and not fix_coord:\n",
    "                                continue\n",
    "\n",
    "                            if p_drop_grad == 0.0 and not fix_grad:\n",
    "                                continue\n",
    "\n",
    "                            if p_drop_delta == 0.0 and not fix_delta:\n",
    "                                continue\n",
    "\n",
    "                            options['n_gac'] = n_gac\n",
    "                            options['p_drop_coord'] = p_drop_coord\n",
    "                            options['fix_drop_coord_over_time'] = fix_coord\n",
    "\n",
    "                            options['p_drop_grad'] = p_drop_grad\n",
    "                            options['p_drop_delta'] = p_drop_delta\n",
    "                            options['fix_drop_delta_over_time'] = fix_delta\n",
    "                            options['fix_drop_grad_over_time'] = fix_grad\n",
    "\n",
    "                            key = \"n_gac={}; p_drop_(coord,grad,delta)=({}, {}, {}), fix_(coord,grad,delta)=({}, {}, {})\".format(n_gac, \n",
    "                                    p_drop_coord, \n",
    "                                    p_drop_grad, \n",
    "                                    p_drop_delta, \n",
    "                                    fix_coord, \n",
    "                                    fix_grad, \n",
    "                                    fix_delta)\n",
    "                            if options.get(key, None) is None:\n",
    "                                lstm_optimizers[key] = LSTMOptimizer(input_var, func, [W, b], **options)\n",
    "                                print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_params(ndim=None):\n",
    "    ndim = ndim or (np.random.randint(low=0, high=50) + 1)\n",
    "    \n",
    "    W = np.cast['float32'](np.random.randn(ndim, ndim))\n",
    "    b = np.cast['float32'](np.random.randn(ndim))\n",
    "    return W, b\n",
    "\n",
    "def sample_point(ndim):\n",
    "    theta = np.cast['float32'](np.random.randn(ndim))\n",
    "    return theta\n",
    "\n",
    "def sample_point_and_params(ndim=None):\n",
    "    W, b = sample_params(ndim)\n",
    "    theta = sample_point(len(b))\n",
    "    \n",
    "    return theta, (W, b)\n",
    "\n",
    "def sample_params_diag(ndim=None):\n",
    "    ndim = ndim or (np.random.randint(low=0, high=50) + 1)\n",
    "    \n",
    "    W = np.cast['float32'](np.diag(np.random.randn(ndim)))\n",
    "    b = np.cast['float32'](np.random.randn(ndim))\n",
    "    return W, b\n",
    "\n",
    "def sample_point_and_params_diag(ndim=10):\n",
    "    W, b = sample_params_diag(ndim)\n",
    "    theta = sample_point(len(b))\n",
    "    \n",
    "    return theta, (np.diag(W), b)\n",
    "\n",
    "# data = [sample_point_and_params(ndim=10) for _ in range(5000)]\n",
    "# data = [sample_point_and_params_diag(100) for _ in range(5000)]\n",
    "# np.random.shuffle(data)\n",
    "# data_index = 0\n",
    "\n",
    "# def sample_from_data():\n",
    "#     global data_index\n",
    "#     theta, (W_, b_) = data[data_index]\n",
    "#     data_index = (data_index + 1) % len(data)\n",
    "#     return theta, (W_, b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_options = {\n",
    "#    'n_iter': 100,\n",
    "    'n_epochs': 50,\n",
    "    'batch_size': 100,\n",
    "    'decay_rate': 0.96,\n",
    "}\n",
    "\n",
    "for key, opt in lstm_optimizers.items():\n",
    "    opt.reset_network()\n",
    "    opt.lr.set_value(0.01)\n",
    "\n",
    "    if key.find('p_drop_coord=0.0') != -1 and key.find('p_drop_delta=0.0') != -1 and key.find('p_drop_grad=0.0') != -1:\n",
    "        sampler = sample_point_and_params\n",
    "    else:\n",
    "        sampler = lambda: sample_point_and_params(ndim=50)\n",
    "    \n",
    "    for n_iter in [20, 40, 60]:\n",
    "        training_options['n_iter'] = n_iter\n",
    "        opt.train(sampler, **training_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_lstm_optimizers = {\n",
    "    'momentum': momentum_fn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_optimizers(**testing_options):\n",
    "    thetas_and_params = [testing_options['sampler']() for _ in range(testing_options['n_functions'])]\n",
    "    \n",
    "    histories = {}\n",
    "    \n",
    "    for key, opt in lstm_optimizers.items():\n",
    "        print(\"Testing lstm; {key}\".format(**locals()))\n",
    "        loss_history = [] \n",
    "        for theta, (W_, b_) in thetas_and_params:\n",
    "            loss_history.append(opt.optimize(theta, [W_, b_], testing_options['n_iter'])[1])\n",
    "        histories['lstm; {}'.format(key)] = np.median(loss_history, axis=0)\n",
    "\n",
    "    lrates = np.logspace(0, 29, num=30, base=2.0) * 1e-6\n",
    "    \n",
    "    for name, opt in non_lstm_optimizers.items():\n",
    "        best_lrate = None\n",
    "        best_loss = None\n",
    "        best_history = None\n",
    "\n",
    "        print(\"Testing {name}\".format(**locals()))\n",
    "        \n",
    "        for lrate in lrates:\n",
    "            loss_history = [] \n",
    "            for theta, (W_, b_) in thetas_and_params:\n",
    "                loss_history.append(opt(theta, testing_options['n_iter'], W_, b_, lrate)[1])\n",
    "            \n",
    "            if np.isnan(loss_history).any():\n",
    "                break\n",
    "            \n",
    "            loss = np.median(loss_history, axis=0)[-1]\n",
    "            if best_loss is None or best_loss > loss:\n",
    "                best_loss = loss\n",
    "                best_lrate = lrate\n",
    "                best_history = np.median(loss_history, axis=0)\n",
    "                \n",
    "        histories[\"{name}; lr={best_lrate}\".format(**locals())] = best_history\n",
    "\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_options = {\n",
    "    'n_iter': 60,\n",
    "    'n_functions': 50,\n",
    "#    'sampler': lambda: sample_point_and_params_diag(np.random.randint(low=0, high=50) + 1)\n",
    "    'sampler': sample_point_and_params\n",
    "}\n",
    "\n",
    "histories_60 = test_optimizers(**testing_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "for name, hist in histories_60.items():\n",
    "    linestyle = 'solid'\n",
    "    plt.semilogy(hist, label=name, linestyle=linestyle)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing_options = {\n",
    "    'n_iter': 500,\n",
    "    'n_functions': 50,\n",
    "#    'sampler': lambda: sample_point_and_params_diag(np.random.randint(low=0, high=50) + 1)\n",
    "    'sampler': sample_point_and_params\n",
    "}\n",
    "\n",
    "histories = test_optimizers(**testing_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "for name, hist in histories.items():\n",
    "    linestyle = 'solid'\n",
    "    plt.semilogy(hist, label=name, linestyle=linestyle)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "generalization_loss_lstm = {name: [] for name in lstm_optimizers}\n",
    "\n",
    "for n_c in itertools.chain(range(2, 50), range(900, 950)):\n",
    "    print(n_c)\n",
    "\n",
    "    points_and_params = [sample_point_and_params(ndim=n_c) for _ in range(10)]\n",
    "    \n",
    "    for name, opt in lstm_optimizers.items():\n",
    "        losses = []\n",
    "        \n",
    "        for theta, (W_, b_) in points_and_params:\n",
    "            loss = opt.optimize(theta, [W_, b_], 100)[1][-1]\n",
    "            losses.append(loss)\n",
    "    \n",
    "        generalization_loss_lstm[name].append(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "for name, losses in generalization_loss_lstm.items():\n",
    "    plt.semilogy(list(range(2, 50)) + list(range(900, 950)), losses[:], label=name)\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, opt in lstm_optimizers.items():\n",
    "    np.savez('final_good_drop_quadratic_optimizer_with_no_error({}).npz'.format(name), L.layers.get_all_param_values(opt.l_optim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.101543426513672\n"
     ]
    }
   ],
   "source": [
    "import theano.sandbox.cuda.basic_ops as sbcuda\n",
    "print(sbcuda.cuda_ndarray.cuda_ndarray.mem_info()[0]/1024./1024/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, opt in lstm_optimizers.items():\n",
    "    with np.load('drop_quadratic_optimizer_({}).npz'.format(name)) as f:\n",
    "        param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    L.layers.set_all_param_values(opt.l_optim, param_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_optimizers_det(**testing_options):\n",
    "    thetas_and_params = [testing_options['sampler']() for _ in range(testing_options['n_functions'])]\n",
    "    \n",
    "    histories = {}\n",
    "    \n",
    "    for key, opt in lstm_optimizers.items():\n",
    "        print(\"Testing lstm; {key}\".format(**locals()))\n",
    "        loss_history = [] \n",
    "        for theta, (W_, b_) in thetas_and_params:\n",
    "            loss_history.append(opt.loss_det_fn(theta, testing_options['n_iter'], W_, b_)[1])\n",
    "        histories['lstm; {}'.format(key)] = np.median(loss_history, axis=0)\n",
    "\n",
    "    lrates = np.logspace(0, 29, num=30, base=2.0) * 1e-6\n",
    "    \n",
    "    for name, opt in non_lstm_optimizers.items():\n",
    "        best_lrate = None\n",
    "        best_loss = None\n",
    "        best_history = None\n",
    "\n",
    "        print(\"Testing {name}\".format(**locals()))\n",
    "        \n",
    "        for lrate in lrates:\n",
    "            loss_history = [] \n",
    "            for theta, (W_, b_) in thetas_and_params:\n",
    "                loss_history.append(opt(theta, testing_options['n_iter'], W_, b_, lrate)[1])\n",
    "            \n",
    "            if np.isnan(loss_history).any():\n",
    "                break\n",
    "            \n",
    "            loss = np.median(loss_history, axis=0)[-1]\n",
    "            if best_loss is None or best_loss > loss:\n",
    "                best_loss = loss\n",
    "                best_lrate = lrate\n",
    "                best_history = np.median(loss_history, axis=0)\n",
    "                \n",
    "        histories[\"{name}; lr={best_lrate}\".format(**locals())] = best_history\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_options = {\n",
    "    'n_iter': 500,\n",
    "    'n_functions': 50,\n",
    "#    'sampler': lambda: sample_point_and_params_diag(np.random.randint(low=0, high=50) + 1)\n",
    "    'sampler': sample_point_and_params\n",
    "}\n",
    "\n",
    "histories_det = test_optimizers_det(**testing_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "for name, hist in histories_det.items():\n",
    "    linestyle = 'solid'\n",
    "    plt.semilogy(hist, label=name, linestyle=linestyle)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "generalization_loss_lstm_2 = {name: [] for name in lstm_optimizers}\n",
    "\n",
    "for n_c in itertools.chain(range(2, 150), range(900, 1000)):\n",
    "    print(n_c)\n",
    "\n",
    "    points_and_params = [sample_point_and_params(ndim=n_c) for _ in range(10)]\n",
    "    \n",
    "    for name, opt in lstm_optimizers.items():\n",
    "        losses = []\n",
    "        \n",
    "        for theta, (W_, b_) in points_and_params:\n",
    "            loss = opt.optimize(theta, [W_, b_], 100)[1][-1]\n",
    "            losses.append(loss)\n",
    "    \n",
    "        generalization_loss_lstm_2[name].append(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "for name, losses in generalization_loss_lstm_2.items():\n",
    "    plt.semilogy(list(range(2, 150)) + list(range(900, 1000)), losses[:], label=name)\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "for name, losses in generalization_loss_lstm_2.items():\n",
    "    plt.semilogy(list(range(2, 150)), losses[:148], label=name)\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "for name, losses in generalization_loss_lstm_2.items():\n",
    "    plt.semilogy(list(range(22, 150)) + list(range(900, 1000)), losses[20:], label=name)\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_optimizers_both(**testing_options):\n",
    "    thetas_and_params = [testing_options['sampler']() for _ in range(testing_options['n_functions'])]\n",
    "    \n",
    "    histories = {}\n",
    "    \n",
    "    for key, opt in lstm_optimizers.items():\n",
    "        print(\"Testing lstm; {key}\".format(**locals()))\n",
    "        loss_history = [] \n",
    "        loss_history_det = [] \n",
    "        for theta, (W_, b_) in thetas_and_params:\n",
    "            loss_history.append(opt.optimize(theta, [W_, b_], testing_options['n_iter'])[1])\n",
    "            loss_history_det.append(opt.loss_det_fn(theta, testing_options['n_iter'], W_, b_)[1])\n",
    "        histories['lstm; {}'.format(key)] = np.median(loss_history, axis=0)\n",
    "        histories['lstm_det; {}'.format(key)] = np.median(loss_history_det, axis=0)\n",
    "\n",
    "    lrates = np.logspace(0, 29, num=30, base=2.0) * 1e-6\n",
    "    \n",
    "    for name, opt in non_lstm_optimizers.items():\n",
    "        best_lrate = None\n",
    "        best_loss = None\n",
    "        best_history = None\n",
    "\n",
    "        print(\"Testing {name}\".format(**locals()))\n",
    "        \n",
    "        for lrate in lrates:\n",
    "            loss_history = [] \n",
    "            for theta, (W_, b_) in thetas_and_params:\n",
    "                loss_history.append(opt(theta, testing_options['n_iter'], W_, b_, lrate)[1])\n",
    "            \n",
    "            if np.isnan(loss_history).any():\n",
    "                break\n",
    "            \n",
    "            loss = np.median(loss_history, axis=0)[-1]\n",
    "            if best_loss is None or best_loss > loss:\n",
    "                best_loss = loss\n",
    "                best_lrate = lrate\n",
    "                best_history = np.median(loss_history, axis=0)\n",
    "                \n",
    "        histories[\"{name}; lr={best_lrate}\".format(**locals())] = best_history\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_options = {\n",
    "    'n_iter': 500,\n",
    "    'n_functions': 50,\n",
    "#    'sampler': lambda: sample_point_and_params_diag(np.random.randint(low=0, high=50) + 1)\n",
    "    'sampler': sample_point_and_params\n",
    "}\n",
    "\n",
    "histories_both = test_optimizers_both(**testing_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "for name, hist in sorted(histories_both.items()):\n",
    "    \n",
    "    if name.find('det') != -1 and name.find('0.0') != -1:\n",
    "        continue\n",
    "    \n",
    "    linestyle = 'solid'\n",
    "    if name.find('det') != -1:\n",
    "        linestyle = '--'\n",
    "    plt.semilogy(hist, label=name, linestyle=linestyle)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_options = {\n",
    "    'n_iter': 500,\n",
    "    'n_functions': 1,\n",
    "#    'sampler': lambda: sample_point_and_params_diag(np.random.randint(low=0, high=50) + 1)\n",
    "    'sampler': sample_point_and_params\n",
    "}\n",
    "\n",
    "histories_both = test_optimizers_both(**testing_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "for name, hist in sorted(histories_both.items()):\n",
    "    \n",
    "    if name.find('det') != -1 and name.find('0.0') != -1:\n",
    "        continue\n",
    "    \n",
    "    linestyle = 'solid'\n",
    "    if name.find('det') != -1:\n",
    "        linestyle = '--'\n",
    "    plt.semilogy(hist, label=name, linestyle=linestyle)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "\n",
    "generalization_loss_lstm_det = {name: [] for name in lstm_optimizers}\n",
    "\n",
    "for n_c in itertools.chain(range(2, 50), range(900, 1000)):\n",
    "    t = time.time()\n",
    "    print(n_c)\n",
    "\n",
    "    points_and_params = [sample_point_and_params(ndim=n_c) for _ in range(10)]\n",
    "    \n",
    "    for name, opt in lstm_optimizers.items():\n",
    "        losses = []\n",
    "        \n",
    "        for theta, (W_, b_) in points_and_params:\n",
    "            loss = opt.loss_det_fn(theta, 100, W_, b_)[1][-1]\n",
    "            losses.append(loss)\n",
    "    \n",
    "        generalization_loss_lstm_det[name].append(np.mean(losses))\n",
    "        \n",
    "    print(\"Time: {}\".format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "for name, losses in generalization_loss_lstm_det.items():\n",
    "    plt.semilogy(list(range(2, 50)) + list(range(900, 1000)), losses, label=name)\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
